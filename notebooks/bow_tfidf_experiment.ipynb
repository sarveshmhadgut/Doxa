{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import scipy\n",
    "import dotenv\n",
    "import mlflow\n",
    "import string\n",
    "import dagshub\n",
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from IPython.display import clear_output\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "CONFIG = {\n",
    "    \"data_path\": \"sample.csv\",\n",
    "    \"test_size\": 0.2,\n",
    "    \"mlflow_tracking_uri\": os.getenv(\"DAGSHUB_URI\"),\n",
    "    \"dagshub_repo_owner\": os.getenv(\"DAGSHUB_USERNAME\"),\n",
    "    \"dagshub_repo_name\": os.getenv(\"DAGSHUB_REPO\"),\n",
    "    \"experiment_name\": \"BoW vs TF-IDF\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea52f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076bbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sample.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e23f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    return re.sub(r\"<.*?>\", \" \", text)\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text=text)\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [w for w in tokens if w not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(w) for w in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88508249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = remove_html(text=text)\n",
    "    text = remove_urls(text=text)\n",
    "    text = remove_punctuations(text=text)\n",
    "    tokens = tokenize(text=text)\n",
    "    tokens = remove_stopwords(tokens=tokens)\n",
    "    tokens = lemmatize_tokens(tokens=tokens)\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"] = df[\"review\"].apply(preprocess_text)\n",
    "df[\"sentiment\"] = df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d06a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(CONFIG[\"mlflow_tracking_uri\"])\n",
    "dagshub.init(\n",
    "    repo_name=CONFIG[\"dagshub_repo_name\"],\n",
    "    repo_owner=CONFIG[\"dagshub_repo_owner\"],\n",
    "    mlflow=True,\n",
    ")\n",
    "mlflow.set_experiment(CONFIG[\"experiment_name\"])\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82747cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = {\n",
    "    \"Bag of Words\": CountVectorizer(max_features=1000),\n",
    "    \"TF-IDF\": TfidfVectorizer(max_features=1000),\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"RandomForest\": RandomForestClassifier(),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8963b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_params(model_name, model):\n",
    "    params_to_log = {}\n",
    "    if model_name == \"LogisticRegression\":\n",
    "        params_to_log[\"C\"] = model.C\n",
    "    elif model_name == \"MultinomialNB\":\n",
    "        params_to_log[\"alpha\"] = model.alpha\n",
    "    elif model_name == \"XGBoost\":\n",
    "        params_to_log[\"n_estimators\"] = model.n_estimators\n",
    "        params_to_log[\"learning_rate\"] = model.learning_rate\n",
    "    elif model_name == \"RandomForest\":\n",
    "        params_to_log[\"n_estimators\"] = model.n_estimators\n",
    "        params_to_log[\"max_depth\"] = model.max_depth\n",
    "    elif model_name == \"GradientBoosting\":\n",
    "        params_to_log[\"n_estimators\"] = model.n_estimators\n",
    "        params_to_log[\"learning_rate\"] = model.learning_rate\n",
    "        params_to_log[\"max_depth\"] = model.max_depth\n",
    "\n",
    "    mlflow.log_params(params_to_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc74d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(df):\n",
    "    with mlflow.start_run(run_name=\"All Experiments\"):\n",
    "        t0 = time.time()\n",
    "        for model_name, model in models.items():\n",
    "            for vectorizer_name, vectorizer in vectorizers.items():\n",
    "                run_name = f\"{model_name} with {vectorizer_name}\"\n",
    "                with mlflow.start_run(run_name=run_name, nested=True):\n",
    "                    logging.info(f\"Starting run: {run_name}...\")\n",
    "                    try:\n",
    "                        vectorizer_instance = vectorizer\n",
    "                        X = vectorizer_instance.fit_transform(df[\"review\"])\n",
    "                        y = df[\"sentiment\"]\n",
    "\n",
    "                        X_train, X_test, y_train, y_test = train_test_split(\n",
    "                            X, y, test_size=CONFIG[\"test_size\"], random_state=42\n",
    "                        )\n",
    "\n",
    "                        mlflow.log_params(\n",
    "                            {\n",
    "                                \"Algorithm\": model_name,\n",
    "                                \"Vectorizer\": vectorizer_name,\n",
    "                                \"test_size\": CONFIG[\"test_size\"],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        model.fit(X_train, y_train)\n",
    "                        y_hat = model.predict(X_test)\n",
    "\n",
    "                        log_model_params(model_name=model_name, model=model)\n",
    "\n",
    "                        mlflow.log_metrics(\n",
    "                            {\n",
    "                                \"accuracy\": accuracy_score(y_test, y_hat),\n",
    "                                \"precision\": precision_score(y_test, y_hat),\n",
    "                                \"recall\": recall_score(y_test, y_hat),\n",
    "                                \"f1_score\": f1_score(y_test, y_hat),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        input_example = (\n",
    "                            X_test[:5]\n",
    "                            if not scipy.sparse.issparse(X_test)\n",
    "                            else X_test[:5]\n",
    "                        )\n",
    "\n",
    "                        mlflow.sklearn.log_model(\n",
    "                            model, \"model\", input_example=input_example\n",
    "                        )\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Unexpected error!: {e}\", exc_info=True)\n",
    "                        mlflow.log_param(\"error\", str(e))\n",
    "\n",
    "                    logging.info(f\"Run '{run_name}' execution complete!\\n\")\n",
    "        t1 = time.time()\n",
    "        logging.info(f\"Execution time : {t1 - t0:.2f} sec\")\n",
    "\n",
    "\n",
    "run_experiments(df)\n",
    "clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doxa (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
