{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cad5c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import scipy\n",
    "import dotenv\n",
    "import mlflow\n",
    "import string\n",
    "import dagshub\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from IPython.display import clear_output\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "941941c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cead019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "for keep in (\"not\", \"no\", \"nor\", \"n't\"):\n",
    "    stop_words.discard(keep)\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4d2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"imdb.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5f849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    return re.sub(r\"<.*?>\", \" \", text)\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text=text)\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [w for w in tokens if w not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(w) for w in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dedd901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = remove_html(text=text)\n",
    "    text = remove_urls(text=text)\n",
    "    text = remove_punctuations(text=text)\n",
    "    tokens = tokenize(text=text)\n",
    "    tokens = remove_stopwords(tokens=tokens)\n",
    "    tokens = lemmatize_tokens(tokens=tokens)\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3577684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"] = df[\"review\"].apply(preprocess_text)\n",
    "df[\"sentiment\"] = df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eec84f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "dagshub_uri = str(os.getenv(\"DAGSHUB_URI\"))\n",
    "dagshub_repo = os.getenv(\"DAGSHUB_REPO\")\n",
    "dagshub_username = os.getenv(\"DAGSHUB_USERNAME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dacd133",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(dagshub_uri)\n",
    "dagshub.init(\n",
    "    repo_name=dagshub_repo,\n",
    "    repo_owner=dagshub_username,\n",
    "    mlflow=True,\n",
    ")\n",
    "mlflow.set_experiment(\"CoSim Hyperparameter Tunning\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61803de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineCentroid(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, shrink_threshold=None):\n",
    "        self.shrink_threshold = shrink_threshold\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = np.array(y)\n",
    "        self.classes_ = np.unique(y)\n",
    "        centroids = []\n",
    "\n",
    "        for c in self.classes_:\n",
    "            Xc = X[y == c]\n",
    "            centroid = Xc.mean(axis=0)\n",
    "\n",
    "            # Sparse → dense 1D vector\n",
    "            if hasattr(centroid, \"toarray\"):\n",
    "                centroid = centroid.toarray().ravel()\n",
    "            else:\n",
    "                centroid = np.asarray(centroid).ravel()\n",
    "\n",
    "            # shrinkage (soft threshold)\n",
    "            if self.shrink_threshold is not None:\n",
    "                t = float(self.shrink_threshold)\n",
    "                centroid = np.sign(centroid) * np.maximum(np.abs(centroid) - t, 0.0)\n",
    "\n",
    "            # L2 normalize centroid\n",
    "            norm = np.linalg.norm(centroid)\n",
    "            if norm > 0:\n",
    "                centroid = centroid / (norm + 1e-12)\n",
    "\n",
    "            centroids.append(centroid)\n",
    "\n",
    "        self.centroids_ = np.vstack(centroids)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Sparse → dense\n",
    "        if hasattr(X, \"toarray\"):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # normalize samples\n",
    "        X_norm = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "        # compute cosine similarity\n",
    "        sim = cosine_similarity(X_norm, self.centroids_)\n",
    "        idx = np.argmax(sim, axis=1)\n",
    "        return self.classes_[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39d5200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "max_iter = 1000\n",
    "max_features = 1000\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "X = vectorizer.fit_transform(df[\"review\"])\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e56916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(df):\n",
    "    clf = CosineCentroid()\n",
    "    param_grid = {\n",
    "        \"shrink_threshold\": [None, 0.0, 0.001, 0.005, 0.01, 0.02, 0.05,\n",
    "                             0.1, 0.2, 0.3, 0.5, 0.7, 1.0,\n",
    "                             2.0, 3.0, 5.0, 10.0]\n",
    "    }\n",
    "\n",
    "    logging.info(\"Starting MLFlow run...\")\n",
    "    with mlflow.start_run(run_name=\"All Experiments\"):\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=clf,\n",
    "                param_grid=param_grid,\n",
    "                cv=5,\n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            for params, mean_score, std_score in zip(\n",
    "                grid_search.cv_results_[\"params\"],\n",
    "                grid_search.cv_results_[\"mean_test_score\"],\n",
    "                grid_search.cv_results_[\"std_test_score\"],\n",
    "            ):\n",
    "                run_name = f\"ShrinkThreshold={params.get('shrink_threshold', 'N/A')}\"\n",
    "\n",
    "                with mlflow.start_run(run_name=run_name, nested=True):\n",
    "                    model = CosineCentroid(**params)\n",
    "                    model.fit(X_train, y_train)\n",
    "\n",
    "                    y_hat = model.predict(X_test)\n",
    "\n",
    "                    mlflow.log_params(params)\n",
    "                    mlflow.log_metrics(\n",
    "                        {\n",
    "                            \"accuracy\": accuracy_score(y_test, y_hat),\n",
    "                            \"precision\": precision_score(y_test, y_hat),\n",
    "                            \"recall\": recall_score(y_test, y_hat),\n",
    "                            \"f1_score\": f1_score(y_test, y_hat),\n",
    "                            \"mean_cv_score\": mean_score,\n",
    "                            \"std_cv_score\": std_score,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    input_example = (\n",
    "                        X_test[:5] if not scipy.sparse.issparse(X_test) else X_test[:5]\n",
    "                    )\n",
    "\n",
    "                    mlflow.sklearn.log_model(\n",
    "                        model, \"model\", input_example=input_example\n",
    "                    )\n",
    "\n",
    "            best_params = grid_search.best_params_\n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_score = grid_search.best_score_\n",
    "\n",
    "            best_y_hat = best_model.predict(X_test)\n",
    "\n",
    "            mlflow.log_params(best_params)\n",
    "            mlflow.log_metric(\"best_cv_f1_score\", best_score)\n",
    "\n",
    "            mlflow.log_metrics(\n",
    "                {\n",
    "                    \"best_accuracy\": accuracy_score(y_test, best_y_hat),\n",
    "                    \"best_precision\": precision_score(y_test, best_y_hat),\n",
    "                    \"best_recall\": recall_score(y_test, best_y_hat),\n",
    "                    \"best_f1_score\": f1_score(y_test, best_y_hat),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            input_example = (\n",
    "                X_test[:5] if not scipy.sparse.issparse(X_test) else X_test[:5]\n",
    "            )\n",
    "\n",
    "            mlflow.sklearn.log_model(\n",
    "                best_model, \"best_model\", input_example=input_example\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error!: {e}\", exc_info=True)\n",
    "            mlflow.log_param(\"error\", str(e))\n",
    "\n",
    "        t1 = time.time()\n",
    "        logging.info(f\"Execution time : {t1 - t0:.2f} sec\")\n",
    "    logging.info(\"MLFlow run execution complete\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93f9fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(df)\n",
    "clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doxa (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
